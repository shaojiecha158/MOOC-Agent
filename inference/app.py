import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import sys

# ================= 1. è·¯å¾„é…ç½® (é€‚é…ä½ åˆšæ‰çš„ç§»åŠ¨æ“ä½œ) =================
# åŸºåº§æ¨¡å‹
BASE_MODEL_PATH = "/gemini/code/model/Qwen2___5-7B-Instruct"
# ä½ çš„å¾®è°ƒæ¨¡å‹ (ä½ åˆšæ‰ mv è¿‡å»çš„ä½ç½®)
LORA_PATH = "/gemini/code/model/my_final_agent"

# ================= 2. æ¨¡å‹åŠ è½½é€»è¾‘ (å’Œä½ ä¹‹å‰çš„ä¸€æ ·ï¼ŒåŠ äº†ç‚¹é˜²æŠ¥é”™) =================
print(f"â³ æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹: {BASE_MODEL_PATH} ...")
try:
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
except Exception as e:
    print(f"âŒ è‡´å‘½é”™è¯¯ï¼šåŸºåº§æ¨¡å‹åŠ è½½å¤±è´¥ã€‚\nåŸå› : {e}")
    sys.exit()

print(f"ğŸ§  æ­£åœ¨æŒ‚è½½å¾®è°ƒå¤§è„‘ (LoRA): {LORA_PATH} ...")
try:
    model = PeftModel.from_pretrained(model, LORA_PATH)
    print("âœ… LoRA æŒ‚è½½æˆåŠŸï¼Agent å·²å°±ç»ªã€‚")
except Exception as e:
    print(
        f"âŒ è‡´å‘½é”™è¯¯ï¼šLoRA åŠ è½½å¤±è´¥ã€‚\nåŸå› : {e}\nè¯·æ£€æŸ¥ '/gemini/code/model/my_final_agent' è¿™ä¸ªæ–‡ä»¶å¤¹é‡Œæœ‰æ²¡æœ‰ adapter_model.safetensors æ–‡ä»¶ã€‚")
    sys.exit()

model.eval()


# ================= 3. å¯¹è¯ç”Ÿæˆé€»è¾‘ (å’Œä½ ä¹‹å‰çš„ä¸€æ ·) =================
def generate_response(message, history):
    # è¿™é‡Œå®šä¹‰å®ƒçš„â€œäººè®¾â€ï¼Œéå¸¸é‡è¦
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ•™è‚²è§„åˆ’å¸ˆ MOOC-Agentã€‚ä½ çš„æ ¸å¿ƒèŒè´£æ˜¯æ ¹æ®ç”¨æˆ·çš„å­¦ä¹ è®°å½•æ¨èè¯¾ç¨‹ã€‚"
        "ã€é‡è¦è§„åˆ™ã€‘"
        "1. å¦‚æœç”¨æˆ·è¾“å…¥çš„æ˜¯å­¦ä¹ è®°å½•ã€è¯¾ç¨‹åæˆ–å­¦ç§‘ç›¸å…³é—®é¢˜ï¼Œè¯·æ ¹æ®çŸ¥è¯†å›¾è°±è§„åˆ’è·¯å¾„ã€‚"
        "   - æœ‰å…ˆä¿®å…³ç³»ç”¨ã€é€»è¾‘è¿è´¯ã€‘ï¼›"
        "   - æ— å…³ç³»ç”¨ã€å†…å®¹æ¨èã€‘ã€‚"
        "2. å¦‚æœç”¨æˆ·é—®çš„æ˜¯æ—¥å¸¸ç”Ÿæ´»ï¼ˆå¦‚çœ‹ç—…ã€å¤©æ°”ï¼‰ã€ä¸“åˆ©ä»£ç æˆ–å…¶ä»–æ— å…³è¯é¢˜ï¼Œè¯·ç›´æ¥å›ç­”ï¼šâ€œæŠ±æ­‰ï¼Œæˆ‘æ˜¯ä¸“æ³¨äºè¯¾ç¨‹æ¨èçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œæ— æ³•å›ç­”è¯¥é¢†åŸŸçš„é€šç”¨é—®é¢˜ã€‚â€"
        "3. ä¸¥ç¦åœ¨éè¯¾ç¨‹æ¨èçš„åœºæ™¯ä¸‹å¼ºè¡Œå¥—ç”¨æ¨èæ¨¡æ¿ã€‚"
    )

    messages = [{"role": "system", "content": system_prompt}]
    for user_msg, bot_msg in history:
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": bot_msg})
    messages.append({"role": "user", "content": message})

    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    with torch.no_grad():
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,
            temperature=0.95,
            top_p=0.85,
            repetition_penalty=1.1,  # æƒ©ç½šå¤è¯»æœºï¼å¼ºåˆ¶å®ƒä¸å‡†é‡å¤ä¸Šä¸€å¥çš„è¯
            do_sample=True,  # ç¡®ä¿å¼€å¯é‡‡æ ·
        )

    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response


# ================= 4. å¯åŠ¨ç•Œé¢ (å¼€å¯ share=True æœ€æ–¹ä¾¿) =================
demo = gr.ChatInterface(
    fn=generate_response,
    title="ğŸ“ MOOC-Agent æ¼”ç¤ºç³»ç»Ÿ",
    description="åŸºäº Qwen2.5 + MOOCCube çŸ¥è¯†å›¾è°±å¾®è°ƒã€‚",
    examples=[
        ["ç”¨æˆ·å·²æŒæ¡: [Cè¯­è¨€ç¨‹åºè®¾è®¡]. ç”¨æˆ·ç›®æ ‡: [æ•°æ®ç»“æ„]."],
        ["ç”¨æˆ·å·²æŒæ¡: [é«˜ç­‰æ•°å­¦]. ç”¨æˆ·ç›®æ ‡: [äººå·¥æ™ºèƒ½]."]
    ],
    theme="soft"
)

if __name__ == "__main__":
    # share=True ä¼šç”Ÿæˆä¸€ä¸ªå…¬å¼€é“¾æ¥ï¼Œä¸ç”¨ç®¡ç«¯å£æ˜ å°„ä¹Ÿèƒ½è®¿é—®
    demo.launch(server_name="0.0.0.0", share=True)